from langchain_groq import ChatGroq
from langchain_text_splitters.character import RecursiveCharacterTextSplitter  
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_mistralai.embeddings import MistralAIEmbeddings
from langchain_community.vectorstores import FAISS 
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from src.stockgpt.components.data_ingestion import DataIngestion
from src.utils.logger import setup_logger
from pathlib import Path
import os
from dotenv import load_dotenv

load_dotenv()

groq_api_key=os.environ['GROQ_API_KEY']
mistral_api_key=os.environ["MISTRAL_API_KEY"]
HF_TOKEN=os.environ["HF_TOKEN"]
logger = setup_logger(__name__, "logs/stockgpt.log")

class RAGHandler:
    """Class for hanlding the RAG Pipeline."""
    def __init__(self):
        """Intializer for the RAG Pipeline"""

        
    def rag_generator(self, query: str, vector_db, web_result):
        """Splits and make chunks of the documents for storage.
        Args:
            query: User's query to the system.
            web_result: web search result.
            documents: Loaded documents of file.
        Returns:
            response: The response generated by the rag pipeline.
        """
        try:
            # creating llm
            llm=ChatGroq(groq_api_key=groq_api_key,
                         model_name="mistral-saba-24b")
            messages = [
                SystemMessagePromptTemplate.from_template(
                    "You are a helpful assistant that only answers based on the given context and web search results if that" \
                    "is relevant to the query."
                ),
                HumanMessagePromptTemplate.from_template(
                    "<context>\n{context}\n</context>\nQuestion: {input}"
                ),
                SystemMessagePromptTemplate.from_template(
                    f"This is the result from web search of this query {{web_result}}"
                ),
                
            ]
            # Creating Prompt template
            prompt = ChatPromptTemplate.from_messages(messages)
            logger.info("Prompt updated for the RAG system.")

            document_chain = create_stuff_documents_chain(llm, prompt)
            logger.info("created document chain.")
            
            retriever=vector_db.as_retriever()
            logger.info("retriever updated for RAG.")

            retrieval_chain=create_retrieval_chain(retriever,document_chain)
            logger.info("created retrieval chain for the RAG pipeline")
            # Invoking the retrieval chain with Input parameters like query and web_result
            response=retrieval_chain.invoke({"input":query,
                                             "web_result":web_result})
            logger.info(f"Data retrieved from document: \n{response['context']} ")
            logger.info(f"response generated for query: {query}")
            
            return response['answer']
        except Exception as e:
            logger.error(f"error while generating for rag: {str(e)}")
            return None
        


            





